<!Doctype html>
<html lang="en">
    <head>
        <title>GRPose: Learning Graph Relations for Human Image Generation with Pose Priors</title>

        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="author" content="Despoina Paschalidou">
        <meta name="viewport" content="width=device-width, initial-scale=1">
<!--        <link rel="icon" type="image/png" href="data/bunny.png"/>-->

          <link rel="stylesheet" href="static/css/bulma.min.css">
<!--          <link rel="stylesheet" href="static/css/bulma-carousel.min.css">-->
<!--          <link rel="stylesheet" href="static/css/bulma-slider.min.css">-->
<!--          <link rel="stylesheet" href="static/css/fontawesome.all.min.css">-->
<!--          <link rel="stylesheet"-->
<!--          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">-->
<!--          <link rel="stylesheet" href="static/css/index.css">-->


        <link rel="stylesheet" type="text/css" href="style_project_page.css?cache=7754391418498779889">
        <link href="https://fonts.googleapis.com/css?family=Arvo|Roboto&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
        <link rel="stylesheet" href="https://unpkg.com/@glidejs/glide/dist/css/glide.core.min.css">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://unpkg.com/@glidejs/glide"></script>

        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <style type="text/css">
            .side-text {
                width:60%;
                display:inline-block;
                vertical-align:top;
            }
            .side-image {
                width: 38%;
                display: inline-block;
                vertical-align: top;
            }
            .controls {
                margin-bottom: 10px;
            }
            .left-controls {
                display: inline-block;
                vertical-align: top;
                width: 80%;
            }
            .right-controls {
                display: inline-block;
                vertical-align: top;
                width: 19%;
                text-align: right;
            }
            .render_window {
                display: inline-block;
                vertical-align: middle;
                box-shadow: 1px 0px 5px black;
                margin-right: 10px;
                margin-bottom: 10px;
                width: calc(33% - 10px);
            }
            .progress {
                background: #666;
                position: relative;
                height: 5px;
                margin-bottom: -5px;
                display: none;
            }
            .glide__slide:hover {cursor: grab;}
            .glide__slide:active {cursor: grabbing;}
            .glide__slide img {width: 90%;}
            .glide__bullets {
                text-align: center;
            }
            .glide__bullet--active {
                color: #aaa; 
            }


            @media (max-width: 400px) {
                .render_window {
                    display: block;
                    width: 90%;
                    margin: 10px auto;
                }
            }
            @media (max-width: 700px) {
                .side-image {
                    display: block;
                    width: 80%;
                    margin: 10px auto;
                }
                .side-text {
                    display: block;
                    width: 100%;
                }
            }
        </style>
    </head>
    <body>
        <div class="section">
<!--            <img src="src\logo_image_final.jpg" style="width:40%;">-->
            
            <h1 class="project-title">
                GRPose: Learning Graph Relations for Human Image Generation with Pose Priors
            </h1>
            
            <div class="authors">

                <a href="">
                    Xiangchen Yin <sup>1</sup>
                </a>
                <a href="">
                    Donglin Di <sup>2</sup>
                </a>
                <a href="">
                    Lei Fan <sup>3</sup>
                </a>
                <a href="">
                    Hao Li <sup>2</sup>
                </a>
                <a href="">
                    Wei Chen <sup>2</sup>
                </a>
                <a href="">
                    Xiaofei Gou <sup>2</sup>
                </a>
                <a href="">
                    Yang Song <sup>3</sup>
                </a>
                <a href="">
                    Xiao Sun <sup>4</sup>
                </a>
                <a href="">
                    Xun Yang <sup>1</sup>
                </a>


            </div>

            <div class="affiliations">
                <span><sup>1</sup> 
                    <a href=https://www.ustc.edu.cn//>
                    University of Science and Technology of China
                    </a>
                </span>

                <span><sup>2</sup> 
                    <a href="">
                    Space AI, Li Auto
                    </a>
                </span>

                <span><sup>3</sup>
                    <a href=https://www.unsw.edu.au//>
                    University of New South Wales
                    </a>
                </span>

                <span><sup>4</sup>
                    <a href=https://www.hfut.edu.cn//>
                    Hefei University of Technology
                    </a>
                </span>

            </div>

            <div class="project-conference">
                AAAI 2025
            </div>

            <div class="project-icons">
                <a href="https://arxiv.org/abs/2408.16540">
                    <i class="fa fa-file"></i> <br/>
                    Paper
                </a>
                <a href="https://github.com/XiangchenYin/GRPose">
                    <i class="fa fa-github"></i> <br/>
                    Code
                </a>

            </div>

            <div class="teaser-image">
                <img src="src\front.png" style="width:90%;">
                <p class="caption" align="center"><strong>GRPose</strong> generates results by better aligning with pose prior and scaling outputs to 512 Ã— 512 pixels. The first row compares it with ControlNet while the second row visualizes the pose alignment across different base models.
                    ControlNet exhibits pose alignment in certain body parts such as arms and incorrect actions like handshakes.
                </p>

            </div>

            
            <div class="section-title">Abstract</div>
            <div class="content">
                <p>Recent methods using diffusion models have made significant progress in human
                    image generation with various additional controls such as pose priors.
                    However, existing approaches still struggle to generate high-quality images with
                    consistent pose alignment, resulting in unsatisfactory outputs. In this paper,
                    we propose a framework delving into the graph relations of pose priors to provide
                    control information for human image generation. </p>
                <p>The main idea is to establish a graph
                    topological structure between the pose priors and latent representation of diffusion
                    models to capture the intrinsic associations between different pose parts.
                    A Progressive Graph Integrator (PGI) is designed to learn the spatial relationships of
                    the pose priors with the graph structure, adopting a hierarchical strategy within an
                    Adapter to gradually propagate information across different pose parts.
                    A pose perception loss is further introduced based on a pretrained pose estimation
                    network to minimize the pose differences. Extensive qualitative and quantitative
                    experiments conducted on the Human-Art and LAION-Human datasets demonstrate that
                    our model achieves superior performance, with a 9.98% increase in pose average precision
                    compared to the latest benchmark model.
                </p>

            </div>


            <div class="section-title">Overview of GRPose</div>
            <div class="content">
                <p>
<!--                    <font color=red>50,000</font> images including more than <font color=red>123,000</font> human figures in <font color=red>20 scenarios</font>-->
                    Our aim is to generate high-quality human images conditioned on pose priors.
                    Our proposed <strong>GRPose</strong> consists of three main components: <strong>Diffusion Pipeline,
                    Graph Pose Adapter and Pose Perception Loss</strong>.
                </p>

                <p>
                    Within the entire framework, the Graph Pose Adapter is a trainable component that encodes the pose condition
                    into a graph structure and integrates it into the Adapter through a hierarchical structure.
                    At the beginning of each encoder layer in the Adapter, the encoded pose and the current latent representation
                    are fed into the <strong>Progressive Graph Integrator (PGI)</strong> to capture the topological relationships between different
                    pose parts through graph learning. This process fine-tunes the Adapter to convey control signals to the SD model,
                    producing the synthesized image. Additionally, to further encourage alignment of the synthesized output with pose
                    priors, the <strong>pose perception loss</strong> is formulated using a pre-trained pose estimation network to quantify the pose
                    differences between outputs and the original images.
                </p>
                <img src="src\overview.png" style="width:90%;">
                <p class="caption" align="center"><strong>Overview of Graph Relation Pose (GRPose).</strong> It consists of Diffusion Pipeline, Graph Pose Adapter and Pose Perception Loss.
                    The Pose Encoder uses a coupling structure in alongside the Progressive Graph Integrator (PGI) to capture graph relationships between different pose parts. The Pose Perception Loss adopts a pre-trained pose estimation network to regularize the pose alignment in the feature space.
                </p> <br>

                <img src="src\PGI.png" style="width:60%;">
                <p class="caption" align="center">
                    <strong>Details of Progressive Graph Integrator (PGI).</strong> The pose prior \( x_p \) and
                    latent representation \( x_l \) are gridded to construct graphs \( \mathcal{G}_p \)
                    and \( \mathcal{G}_p \) respectively, where GCNs are employed to fuse and update the information.
                </p>

                <div class="section-title">Visualization of Methods</div>

                <div class="glide1">
                    <div class="glide__bullets" data-glide-el="controls[nav]">
                        <button class="glide__bullet" data-glide-dir="=0">Comparison</button>
                        <button class="glide__bullet" data-glide-dir="=1">Tranfer</button>
                        <button class="glide__bullet" data-glide-dir="=2">Multi-Person</button>
                    </div>
                    <div class="glide__track" data-glide-el="track">
                        <ul class="glide__slides">
                            <li class="glide__slide"><img src="src/compa.png" style="width:70%;">
                                <p class="caption" align="center">
                                    <strong>Visual comparison of ours and other methods.</strong> The samples are from the Human-Art dataset, with each row representing a sample along with its corresponding pose and prompt. Our GRPose, HumanSD, ControlNet, and T2I-Adapter were provided with both a prompt and a pose, while SD1.5 was provided with only a prompt. Our model achieved outstanding results in visual quality and pose alignment.                                </p>
                            </li>

                            <li class="glide__slide"><img src="src/transfer.png" style="width:60%;">
                                <p class="caption" align="center">
                                    <strong>Comparison of different base diffusion models for our GRPose.</strong> We compared SD1.5, Anime Art and Realistic models of different styles. GRPose effectively enhances each model with superior pose alignment.                                </p>
                            </li>

                            <li class="glide__slide"><img src="src/multi-pose.png" style="width:60%;">
                                <p class="caption" align="center">
                                    <strong>Results of Multi-Pose Generation.</strong> Our model outperforms ControlNet in generating multiple poses.
                                </p>
                            </li>

                        </ul>
                    </div>
                </div>

                <br>

            <div class="section-title">Quantitative Results</div>
            <div class="content">
                <img src="src\table.png" style="width:80%;">
                <p class="caption" align="center"><strong>Results on Human-Art and LAION-Human datasets.</strong> The best results and the second best results are marked in green and blue respectively. Results marked with asterisk (*) are evaluated on the released models.</p>
            </div>



          <div class="section-title">BibTeX</div>
            <div class="container is-max-desktop content">
              <pre><code>@article{yin2024grpose,
          title={Grpose: Learning graph relations for human image generation with pose priors},
          author={Yin, Xiangchen and Di, Donglin and Fan, Lei and Li, Hao and Wei, Chen and Gou, Xiaofei and Song, Yang and Sun, Xiao and Yang, Xun},
          journal={arXiv preprint arXiv:2408.16540},
          year={2024}
        }</code></pre>
            </div>

</section>






        <script type="module">
            import * as THREE from "https://unpkg.com/three@0.127.0/build/three.module.js";
            import {OrbitControls} from "https://unpkg.com/three@0.127.0/examples/jsm/controls/OrbitControls.js";
            import {OBJLoader} from "https://unpkg.com/three@0.127.0/examples/jsm/loaders/OBJLoader.js";

            // Render the predictions
            function random_choice(arr, n) {
                var index_set = {};
                var choice = [];
                while (choice.length < n) {
                    var idx = Math.floor(Math.random() * arr.length);
                    if (index_set[idx] !== undefined) {
                        continue;
                    }
                    index_set[idx] = 0;
                    choice.push(idx);
                }

                return choice.map(x => arr[x]);
            }

            function progress_bar() {
                var el = document.createElement("div");
                el.classList.add("progress");

                return {
                    domElement: el,
                    update: function (percent) {
                        percent = Math.min(1, Math.max(0, percent));
                        el.style.display = "block";
                        el.style.width = Math.round(percent * 100) + "%";
                    },
                    hide: function () {
                        el.style.display = "none";
                    }
                };
            }

            function reset_checkboxes(checkboxes) {
                Array.prototype.forEach.call(checkboxes, function (c) {
                    c.checked = false;
                });
                checkboxes[0].checked = true;
            }

            function show_object(el, prefix, N) {
                const scene = new THREE.Scene();
                const renderer = new THREE.WebGLRenderer();
                const camera = new THREE.PerspectiveCamera(75, 1, 0.1, 1000);
                const controls = new OrbitControls(camera, renderer.domElement);

                camera.position.set(0.5, 0.5, 0.5);
                controls.target.set(0, 0, 0);
                controls.autoRotate = true;
                controls.autoRotateSpeed = 4;
                scene.background = new THREE.Color("white");
                var size = el.dataset.size;
                renderer.setSize(size, size);
                var progress = progress_bar();
                el.appendChild(progress.domElement);
                el.appendChild(renderer.domElement);

                const amb_light = new THREE.AmbientLight(0x606060); // soft white light
                scene.add(amb_light);
                const hem_light = new THREE.HemisphereLight(0xffffbb, 0x080820, 0.5);
                scene.add(hem_light);

                const colors = [
                    0x1f77b4,
                    0xaec7e8,
                    0xff7f0e,
                    0xffbb78,
                    0x2ca02c,
                    0x98df8a,
                    0xd62728,
                    0xff9896,
                    0x9467bd,
                    0xc5b0d5,
                    0x8c564b,
                    0xc49c94,
                    0xe377c2,
                    0xf7b6d2,
                    0x7f7f7f,
                    0xc7c7c7,
                    0xbcbd22,
                    0xdbdb8d,
                    0x17becf,
                    0x9edae5
                ];
                var previous_canvas_size = size;
                function animate() {
                    requestAnimationFrame(animate);
                    if (el.offsetWidth != previous_canvas_size) {
                        previous_canvas_size = el.offsetWidth;
                        renderer.domElement.style.width = previous_canvas_size + "px";
                        renderer.domElement.style.height = previous_canvas_size + "px";
                    }

                    controls.update();
                    renderer.render(scene, camera);
                }

                const loader = new OBJLoader();
                var meshes = [];
                var progresses = [];
                var loaded = 0;
                function load_part(part_idx) {
                    progresses[part_idx] = 0;
                    loader.load(
                        prefix + "/part_00" + i + ".obj",
                        function (object) {
                            var g = object.children[0].geometry;
                            var m = new THREE.MeshLambertMaterial({color: colors[part_idx]});
                            m.transparent = true;
                            var mesh = new THREE.Mesh(g, m);
                            meshes[part_idx] = mesh;
                            scene.add(mesh);

                            loaded++;
                            if (loaded == N) {
                                progress.hide();
                            }
                        },
                        function (event) {
                            progresses[part_idx] = event.loaded / event.total;
                            var total_progress = 0;
                            for (var i=0; i<progresses.length; i++) {
                                total_progress += progresses[i] / progresses.length;
                            }
                            progress.update(total_progress);
                        }
                    )
                }
                for (var i=0; i<N; i++) {
                    load_part(i);
                }
                animate();

                return {
                    meshes: meshes,
                    show: function (indices) {
                        for (var i=0; i<N; i++) {
                            meshes[i].material.opacity = 0.5;
                            //meshes[i].visible = false;
                        }
                        for (var i=0; i<indices.length; i++) {
                            meshes[indices[i]].material.opacity = 1;
                            //meshes[indices[i]].visible = true;
                        }
                    },
                    show_all: function () {
                        for (var i=0; i<N; i++) {
                            meshes[i].material.opacity = 1;
                            //meshes[i].visible = true;
                        }
                    },
                    set_size: function(width, height) {
                        renderer.setSize(width, height);
                    }
                };
            }

            function show_group(elements, objects, N) {
                var controls = [];
                for (var i=0; i<objects.length; i++) {
                    controls.push(show_object(elements[i], objects[i], N));
                }

                return {
                    controls: controls,
                    show: function (indices) {
                        for (var i=0; i<controls.length; i++) {
                            controls[i].show(indices);
                        }
                    },
                    show_all: function () {
                        for (var i=0; i<controls.length; i++) {
                            controls[i].show_all();
                        }
                    }
                };
            }

            // Humans
            var humans = [
                "https://superquadrics.com/neural_parts/humans/50002_chicken_wings",
                "https://superquadrics.com/neural_parts/humans/50002_hips",
                "https://superquadrics.com/neural_parts/humans/50002_jiggle_on_toes",
                "https://superquadrics.com/neural_parts/humans/50002_jumping_jacks",
                "https://superquadrics.com/neural_parts/humans/50002_jumping_jacks_00038",
                "https://superquadrics.com/neural_parts/humans/50002_jumping_jacks_2",
                "https://superquadrics.com/neural_parts/humans/50002_knees",
                "https://superquadrics.com/neural_parts/humans/50004_punching",
                "https://superquadrics.com/neural_parts/humans/50004_running_on_spot",
                "https://superquadrics.com/neural_parts/humans/50004_running_on_spot_00220",
                "https://superquadrics.com/neural_parts/humans/50004_running_on_spot_2",
                "https://superquadrics.com/neural_parts/humans/50004_running_spot",
                "https://superquadrics.com/neural_parts/humans/50007_jumping_jacks",
                "https://superquadrics.com/neural_parts/humans/50009_chicken_wings",
                "https://superquadrics.com/neural_parts/humans/50009_jumping_jacks",
                "https://superquadrics.com/neural_parts/humans/50009_jumping_jacks_00140",
                "https://superquadrics.com/neural_parts/humans/50009_one_leg_jump",
                "https://superquadrics.com/neural_parts/humans/50009_one_leg_jump_00075",
                "https://superquadrics.com/neural_parts/humans/50009_shake_hips",
                "https://superquadrics.com/neural_parts/humans/50020_knees_00136",
                "https://superquadrics.com/neural_parts/humans/50021_knees",
                "https://superquadrics.com/neural_parts/humans/50021_knees_2",
                "https://superquadrics.com/neural_parts/humans/50021_knees_3",
                "https://superquadrics.com/neural_parts/humans/50021_knees_4",
                "https://superquadrics.com/neural_parts/humans/50021_knees_5",
                "https://superquadrics.com/neural_parts/humans/50021_knees_6",
                "https://superquadrics.com/neural_parts/humans/50021_knees_7",
                "https://superquadrics.com/neural_parts/humans/50021_one_leg_jump",
                "https://superquadrics.com/neural_parts/humans/50021_running_on_spot",
                "https://superquadrics.com/neural_parts/humans/50021_running_on_spot_2",
                "https://superquadrics.com/neural_parts/humans/50022_punching_00069",
                "https://superquadrics.com/neural_parts/humans/50022_shake_hips",
                "https://superquadrics.com/neural_parts/humans/50026_knees",
                "https://superquadrics.com/neural_parts/humans/50027_jumping_jacks",
                "https://superquadrics.com/neural_parts/humans/50027_jumping_jacks_2",
                "https://superquadrics.com/neural_parts/humans/50027_jumping_jacks_3",
                "https://superquadrics.com/neural_parts/humans/50027_jumping_jacks_4",
                "https://superquadrics.com/neural_parts/humans/50027_jumping_jacks_5",
            ];
            var human_control = show_group(
                document.getElementById("humans").getElementsByClassName("render_window"),
                [humans[0], humans[1], humans[2]],
                6
            );
            var human_checkboxes = document.querySelectorAll("#humans .controls input");
            reset_checkboxes(human_checkboxes);
            document.querySelector("#humans .controls").addEventListener(
                "change",
                function (ev) {
                    if (ev.target.id == "humans_all") {
                        Array.prototype.filter.call(
                            human_checkboxes,
                            (el) => el.id != "humans_all"
                        ).forEach(function (el) {el.checked = false;});
                    } else if (ev.target.checked) {
                        human_checkboxes[0].checked = false;
                    }

                    var ids = new Set();
                    if (human_checkboxes[0].checked) {
                        ids = new Set([0, 1, 2, 3, 4, 5]);
                    }
                    var part_ids = [1, 2, 0, 4, 3, 5];
                    for (var i=1; i<human_checkboxes.length; i++) {
                        if (human_checkboxes[i].checked) {
                            ids.add(part_ids[i-1]);
                        }
                    }

                    human_control.show(Array.from(ids));
                }
            );
            document.querySelector("#humans .controls button").addEventListener(
                "click",
                function (ev) {
                    reset_checkboxes(human_checkboxes);
                    var new_humans = random_choice(humans, 3);
                    var render_windows = document.getElementById("humans").getElementsByClassName("render_window");
                    Array.prototype.forEach.call(render_windows, function (r) {r.innerHTML = "";});
                    human_control = show_group(
                        render_windows,
                        new_humans,
                        6
                    );
                }
            );

            // Planes
            var planes = [
                "https://superquadrics.com/neural_parts/planes/10af5de930178a161596c26b5af806fe",
                "https://superquadrics.com/neural_parts/planes/1a32f10b20170883663e90eaf6b4ca52",
                "https://superquadrics.com/neural_parts/planes/1a6ad7a24bb89733f412783097373bdc",
                "https://superquadrics.com/neural_parts/planes/1b3c6b2fbcf834cf62b600da24e0965",
                "https://superquadrics.com/neural_parts/planes/1c26ecb4cd01759dc1006ed55bc1a3fc",
                "https://superquadrics.com/neural_parts/planes/284e6431669d46fd44797ce00623b3fd",
                "https://superquadrics.com/neural_parts/planes/2c3ba3f35c5d2b0ce77e43d0a92bdc06",
                "https://superquadrics.com/neural_parts/planes/315f523d0a924fb7ef70df8610b582b2",
                "https://superquadrics.com/neural_parts/planes/343a607d1604335fb4f192eea1889928",
                "https://superquadrics.com/neural_parts/planes/347d86d7001cef01232236eecec447b",
                "https://superquadrics.com/neural_parts/planes/351c9235749e398162147e00e97e28b5",
                "https://superquadrics.com/neural_parts/planes/3716ed4fa80dbf5f41392ab7a601818b",
                "https://superquadrics.com/neural_parts/planes/384e72f69e6f24404cb288947cda4a2c",
                "https://superquadrics.com/neural_parts/planes/440ac1b4ac3cbe114c3a35cee92bb95b",
                "https://superquadrics.com/neural_parts/planes/440e5ba74ac8124e9751c7a6f15617f4",
                "https://superquadrics.com/neural_parts/planes/48706d323b9041d5438a95791ca4064d",
                "https://superquadrics.com/neural_parts/planes/563cef4df464ddb1e153dd90dac45a6d",
                "https://superquadrics.com/neural_parts/planes/5c6590461085c93ea91e80f26309099e",
                "https://superquadrics.com/neural_parts/planes/60b5f5da40e0dd33579f6385fdd4245b",
                "https://superquadrics.com/neural_parts/planes/7b134f6573e7270fb0a79e28606cb167",
                "https://superquadrics.com/neural_parts/planes/92a83ecaa10e8d3f78e919a72d9a39e7",
                "https://superquadrics.com/neural_parts/planes/ed2aaca045fb1714cd4229f38ad0d015",
                "https://superquadrics.com/neural_parts/planes/f12eefbbefabe566ca8607f540cc62ba",
            ];
            var plane_control = show_group(
                document.getElementById("planes").getElementsByClassName("render_window"),
                [planes[7], planes[1], planes[2]],
                5
            );
            var plane_checkboxes = document.querySelectorAll("#planes .controls input");
            reset_checkboxes(plane_checkboxes);
            document.querySelector("#planes .controls").addEventListener(
                "change",
                function (ev) {
                    if (ev.target.id == "planes_all") {
                        Array.prototype.filter.call(
                            plane_checkboxes,
                            (el) => el.id != "planes_all"
                        ).forEach(function (el) {el.checked = false;});
                    } else if (ev.target.checked) {
                        plane_checkboxes[0].checked = false;
                    }

                    var ids = new Set();
                    if (plane_checkboxes[0].checked) {
                        ids = new Set([0, 1, 2, 3, 4]);
                    }
                    var part_ids = [4, 0, 3, 2, 1];
                    for (var i=1; i<plane_checkboxes.length; i++) {
                        if (plane_checkboxes[i].checked) {
                            ids.add(part_ids[i-1]);
                        }
                    }

                    plane_control.show(Array.from(ids));
                }
            );
            document.querySelector("#planes .controls button").addEventListener(
                "click",
                function (ev) {
                    reset_checkboxes(plane_checkboxes);
                    var new_planes = random_choice(planes, 3);
                    var render_windows = document.getElementById("planes").getElementsByClassName("render_window");
                    Array.prototype.forEach.call(render_windows, function (r) {r.innerHTML = "";});
                    plane_control = show_group(
                        render_windows,
                        new_planes,
                        5
                    );
                }
            );
        </script>
        <script>
            // Make the carousel for the comparisons
            var glide1 = new Glide(".glide1", {
                type: "carousel",
                startAt: 0,
                perView: 1,
                autoplay: 4000
            }).mount();
        </script>
        <script>
            // Make the carousel for the comparisons
            var glide2 = new Glide(".glide2", {
                type: "carousel",
                startAt: 0,
                perView: 1,
                autoplay: 4000
            }).mount();
        </script>
        <script>
            // Make the carousel for the comparisons
            var glide3 = new Glide(".glide3", {
                type: "carousel",
                startAt: 0,
                perView: 1,
                autoplay: 4000
            }).mount();
        </script>
    </body>
</html>
